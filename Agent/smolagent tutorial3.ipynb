{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3-l3Imyy11b"
      },
      "outputs": [],
      "source": [
        "!pip install -q smolagents\n",
        "!pip install -q huggingface_hub\n",
        "!pip -q install litellm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYswvuS7y-RF"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import login\n",
        "\n",
        "# login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NW4b6Z5EzPWF"
      },
      "outputs": [],
      "source": [
        "from smolagents import CodeAgent, DuckDuckGoSearchTool, HfApiModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EkErpzYBzxQ"
      },
      "outputs": [],
      "source": [
        "# from smolagents import CodeAgent, HfApiModel\n",
        "# from huggingface_hub import login\n",
        "\n",
        "# login(\"<YOUR_HUGGINGFACEHUB_API_TOKEN>\")\n",
        "\n",
        "# model_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
        "\n",
        "# model = HfApiModel(model_id=model_id)\n",
        "# agent = CodeAgent(tools=[], model=model, add_base_tools=True)\n",
        "\n",
        "# agent.run(\n",
        "#     \"Could you give me the 118th number in the Fibonacci sequence?\",\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaiRr4d5FBWl"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('openai')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "zr_eT8zqEyFx",
        "outputId": "82cbd14b-3bcd-4019-b398-0eed89e85044"
      },
      "outputs": [],
      "source": [
        "from smolagents import CodeAgent, DuckDuckGoSearchTool, LiteLLMModel\n",
        "\n",
        "model = LiteLLMModel(model_id=\"gpt-4o\")\n",
        "\n",
        "agent = CodeAgent(tools=[], model=model, add_base_tools=True)\n",
        "\n",
        "agent.run(\"Could you give me the 118th number in the Fibonacci sequence?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFRFI3djGCQC"
      },
      "source": [
        "### Code execution\n",
        "A Python interpreter executes the code on a set of inputs passed along with your tools. This should be safe because the only functions that can be called are the tools you provided (especially if it’s only tools by Hugging Face) and a set of predefined safe functions like print or functions from the math module, so you’re already limited in what can be executed.\n",
        "\n",
        "The Python interpreter also doesn’t allow imports by default outside of a safe list, so all the most obvious attacks shouldn’t be an issue. You can authorize additional imports by passing the authorized modules as a list of strings in argument additional_authorized_imports upon initialization of your CodeAgent or CodeAgent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BCBI-k1WFmvl",
        "outputId": "94ea5f26-9793-49f0-c7db-da4dd6151473"
      },
      "outputs": [],
      "source": [
        "# access to aditional python packages\n",
        "\n",
        "from smolagents import CodeAgent\n",
        "\n",
        "agent = CodeAgent(tools=[], model=model)\n",
        "agent.run(\"Could you get me the title of the page at url 'https://huggingface.co/blog'?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "id": "ilxAbDHWzYIN",
        "outputId": "5016be2b-e930-4105-9142-a2899982e04a"
      },
      "outputs": [],
      "source": [
        "agent = CodeAgent(tools=[], model=model, additional_authorized_imports=['requests', 'bs4'])\n",
        "agent.run(\"Could you get me the title of the page at url 'https://huggingface.co/blog'?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2N1w7DSwGMm0"
      },
      "source": [
        "### System Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6eqX6dDGH-s",
        "outputId": "09ce7efa-b3ed-4df9-ae7c-7e17128d1fe2"
      },
      "outputs": [],
      "source": [
        "print(agent.system_prompt_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn_1qCAFGe1O"
      },
      "source": [
        "The system prompt includes:\n",
        "\n",
        "- An introduction that explains how the agent should behave and what tools are.\n",
        "- A description of all the tools that is defined by a {{tool_descriptions}} token that is dynamically replaced at runtime with the tools defined/chosen by the user.\n",
        "- The tool description comes from the tool attributes, name, description, inputs and output_type, and a simple jinja2 template that you can refine.\n",
        "- The expected output format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7i-s99LG8hL"
      },
      "source": [
        "### Updating the System Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Itlv6oulGIBy"
      },
      "outputs": [],
      "source": [
        "from smolagents import ToolCallingAgent, PythonInterpreterTool, TOOL_CALLING_SYSTEM_PROMPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqWI_uWKGIEq",
        "outputId": "e4af9171-3488-41eb-83e1-49a88eca1e5d"
      },
      "outputs": [],
      "source": [
        "print(TOOL_CALLING_SYSTEM_PROMPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ewqmr6QOGIHy"
      },
      "outputs": [],
      "source": [
        "modified_prompt = TOOL_CALLING_SYSTEM_PROMPT\n",
        "\n",
        "agent = ToolCallingAgent(tools=[PythonInterpreterTool()], model=model, system_prompt=modified_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzZOilXQGILK",
        "outputId": "1765efff-17d1-4a2f-f1f5-6df7e25380fa"
      },
      "outputs": [],
      "source": [
        "print(agent.system_prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "J3btCWu3GIOT",
        "outputId": "606c6d94-e263-4609-980c-fcb2f26660e8"
      },
      "outputs": [],
      "source": [
        "agent.run(\"How many seconds would it take for a leopard at full speed to run through Pont des Arts?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FyDpYv_GIRr",
        "outputId": "b0f6107a-eedc-4212-d11f-8df479463013"
      },
      "outputs": [],
      "source": [
        "print(agent.logs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XKfjyX0UhMS",
        "outputId": "95a7ff6b-2341-4027-bb17-a389bc267d67"
      },
      "outputs": [],
      "source": [
        "agent.logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euklcIX1GIU2",
        "outputId": "bebdd09d-194f-41b3-9c45-570d803c2f3f"
      },
      "outputs": [],
      "source": [
        "agent.write_inner_memory_from_logs()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOciConxtuEx"
      },
      "source": [
        "### Tools\n",
        "\n",
        "A tool is an atomic function to be used by an agent. To be used by an LLM, it also needs a few attributes that constitute its API and will be used to describe to the LLM how to call this tool:\n",
        "\n",
        "- A name\n",
        "- A description\n",
        "- Input types and descriptions\n",
        "- An output type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWBMkn8LuHIL"
      },
      "source": [
        "#### Default toolbox\n",
        "Transformers comes with a default toolbox for empowering agents, that you can add to your agent upon initialization with argument add_base_tools = True:\n",
        "\n",
        "- DuckDuckGo web search*: performs a web search using DuckDuckGo browser.\n",
        "- Python code interpreter: runs your the LLM generated Python code in a secure environment. This tool will only be added to ToolCallingAgent if you initialize it with add_base_tools=True, since code-based agent can already natively execute Python code\n",
        "- Transcriber: a speech-to-text pipeline built on Whisper-Turbo that transcribes an audio to text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYQ5s9xyupW1"
      },
      "source": [
        "#### Create a new tool\n",
        "You can create your own tool for use cases not covered by the default tools from Hugging Face. For example, let’s create a tool that returns the most downloaded model for a given task from the Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONE9cDW1tonh",
        "outputId": "6b30b366-01d6-4b2d-8a43-09f9a366954a"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import list_models\n",
        "\n",
        "task = \"text-classification\"\n",
        "\n",
        "most_downloaded_model = next(iter(list_models(filter=task, sort=\"downloads\", direction=-1)))\n",
        "\n",
        "print(most_downloaded_model.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a67lh5RXvix2",
        "outputId": "cc73e334-69bb-40c4-fbc9-495ab6274270"
      },
      "outputs": [],
      "source": [
        "task = \"text-to-video\"\n",
        "\n",
        "most_downloaded_model = next(iter(list_models(filter=task, sort=\"downloads\", direction=-1)))\n",
        "\n",
        "print(most_downloaded_model.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWjI-qy7toq7"
      },
      "outputs": [],
      "source": [
        "from transformers import tool\n",
        "\n",
        "@tool\n",
        "def model_download_tool(task: str) -> str:\n",
        "    \"\"\"\n",
        "    This is a tool that returns the most downloaded model of a given task on the Hugging Face Hub.\n",
        "    It returns the name of the checkpoint.\n",
        "\n",
        "    Args:\n",
        "        task: The task for which\n",
        "    \"\"\"\n",
        "    most_downloaded_model = next(iter(list_models(filter=task, sort=\"downloads\", direction=-1)))\n",
        "    return most_downloaded_model.id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG2U8ffpu9t8"
      },
      "source": [
        "### The function needs:\n",
        "\n",
        "- A clear name. The name usually describes what the tool does. Since the code returns the model with the most downloads for a task, let’s put model_download_tool.\n",
        "- Type hints on both inputs and output\n",
        "- A description, that includes an ‘Args:’ part where each argument is described (without a type indication this time, it will be pulled from the type hint). All these will be automatically baked into the agent’s system prompt upon initialization: so strive to make them as clear as possible!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "kqYLXbPutouT",
        "outputId": "4cebdab6-566b-4bb6-91c7-83aab307c8de"
      },
      "outputs": [],
      "source": [
        "from smolagents import CodeAgent, HfApiModel\n",
        "\n",
        "agent = CodeAgent(tools=[model_download_tool], model=model)\n",
        "\n",
        "agent.run(\"Can you give me the name of the model that has the most downloads in the 'text-to-video' task on the Hugging Face Hub?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "id": "Vf2Fnp8Tto0k",
        "outputId": "6115b235-d03c-475c-88a7-24fbc77f47d3"
      },
      "outputs": [],
      "source": [
        "from smolagents import (\n",
        "    load_tool,\n",
        "    CodeAgent,\n",
        "    HfApiModel,\n",
        "    GradioUI\n",
        ")\n",
        "\n",
        "# Import tool from Hub\n",
        "image_generation_tool = load_tool(\"m-ric/text-to-image\", trust_remote_code=True)\n",
        "\n",
        "# model = HfApiModel(model_id)\n",
        "\n",
        "# Initialize the agent with the image generation tool\n",
        "agent = CodeAgent(tools=[image_generation_tool], model=model)\n",
        "\n",
        "GradioUI(agent).launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn3VDbucxM0S"
      },
      "source": [
        "## Orchestrate a multi-agent system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wriaVr9YxOKP"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import requests\n",
        "from markdownify import markdownify\n",
        "from requests.exceptions import RequestException\n",
        "from smolagents import tool\n",
        "\n",
        "\n",
        "@tool\n",
        "def visit_webpage(url: str) -> str:\n",
        "    \"\"\"Visits a webpage at the given URL and returns its content as a markdown string.\n",
        "\n",
        "    Args:\n",
        "        url: The URL of the webpage to visit.\n",
        "\n",
        "    Returns:\n",
        "        The content of the webpage converted to Markdown, or an error message if the request fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Send a GET request to the URL\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "        # Convert the HTML content to Markdown\n",
        "        markdown_content = markdownify(response.text).strip()\n",
        "\n",
        "        # Remove multiple line breaks\n",
        "        markdown_content = re.sub(r\"\\n{3,}\", \"\\n\\n\", markdown_content)\n",
        "\n",
        "        return markdown_content\n",
        "\n",
        "    except RequestException as e:\n",
        "        return f\"Error fetching the webpage: {str(e)}\"\n",
        "    except Exception as e:\n",
        "        return f\"An unexpected error occurred: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtcodw6Sxfgb",
        "outputId": "4db6ca67-37fa-4376-a03a-b9f90e431d86"
      },
      "outputs": [],
      "source": [
        "print(visit_webpage(\"https://en.wikipedia.org/wiki/Hugging_Face\")[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3R0Pg9sxiFv"
      },
      "outputs": [],
      "source": [
        "from smolagents import (\n",
        "    CodeAgent,\n",
        "    ToolCallingAgent,\n",
        "    HfApiModel,\n",
        "    ManagedAgent,\n",
        "    DuckDuckGoSearchTool,\n",
        "    LiteLLMModel\n",
        ")\n",
        "\n",
        "# model = HfApiModel(model_id)\n",
        "\n",
        "web_agent = ToolCallingAgent(\n",
        "    tools=[DuckDuckGoSearchTool(), visit_webpage],\n",
        "    model=model,\n",
        "    # max_iterations=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xQBOOBaxyCI"
      },
      "outputs": [],
      "source": [
        "managed_web_agent = ManagedAgent(\n",
        "    agent=web_agent,\n",
        "    name=\"search\",\n",
        "    description=\"Runs web searches for you. Give it your query as an argument.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpljjRCLx7IE"
      },
      "outputs": [],
      "source": [
        "manager_agent = CodeAgent(\n",
        "    tools=[],\n",
        "    model=model,\n",
        "    managed_agents=[managed_web_agent],\n",
        "    additional_authorized_imports=[\"time\", \"numpy\", \"pandas\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YJGW_h5NyD1q",
        "outputId": "3df40bac-c9ad-4d23-f7f6-17149e28c071"
      },
      "outputs": [],
      "source": [
        "answer = manager_agent.run(\"Which dynasty was ruling China at the time of the fall of Constantinople?.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UR5fq50fyIjR",
        "outputId": "46e4edee-f9c4-4a32-c222-5f4dc8343674"
      },
      "outputs": [],
      "source": [
        "answer = manager_agent.run(\"Which dynasty was ruling China at the time of the fall of Constantinople?.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yn_0MDJIy9VH",
        "outputId": "a94abfe9-8ac8-4f62-d6f8-a01410febb93"
      },
      "outputs": [],
      "source": [
        "answer = manager_agent.run(\"If LLM trainings continue to scale up at the current rythm until 2030, what would be the electric power in GW required to power the biggest training runs by 2030? What does that correspond to, compared to some contries? Please provide a source for any number used.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
