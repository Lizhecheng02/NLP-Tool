{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47727,
     "status": "ok",
     "timestamp": 1728564025916,
     "user": {
      "displayName": "Sam Witteveen",
      "userId": "13451642680591748340"
     },
     "user_tz": -480
    },
    "id": "e5cOHvMEI_vG",
    "outputId": "f16a464c-b0de-444d-e40d-b2058639de73"
   },
   "outputs": [],
   "source": [
    "!pip -q install gradio langchain-openai langchain-community langchain langchain-google-genai langchain-anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WYH28kmuJTNq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = userdata.get('ANTHROPIC_API_KEY')\n",
    "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_AI_STUDIO')\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OGKoUYrXJpwW"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2fvcoLxWdvS"
   },
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant who acts like a pirate.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "executionInfo": {
     "elapsed": 1130059,
     "status": "ok",
     "timestamp": 1728567305002,
     "user": {
      "displayName": "Sam Witteveen",
      "userId": "13451642680591748340"
     },
     "user_tz": -480
    },
    "id": "GQth439KJGG_",
    "outputId": "e8e72ac2-b1bc-4035-97ac-22413f788787"
   },
   "outputs": [],
   "source": [
    "# Initialize chat model\n",
    "# llm = ChatOpenAI(temperature=0.7, model='gpt-4o-mini', streaming=True)\n",
    "\n",
    "# Initialize Gemini AI Studio chat model\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-002\", streaming=True)\n",
    "\n",
    "# Initialize Gemini AI Studio chat model\n",
    "llm = ChatAnthropic(model='claude-3-haiku-20240307', streaming=True)\n",
    "\n",
    "\n",
    "def stream_response(message, history):\n",
    "    print(f\"Input: {message}. History: {history}\\n\")\n",
    "\n",
    "    history_langchain_format = []\n",
    "    history_langchain_format.append(SystemMessage(content=system_message))\n",
    "\n",
    "    for human, ai in history:\n",
    "        history_langchain_format.append(HumanMessage(content=human))\n",
    "        history_langchain_format.append(AIMessage(content=ai))\n",
    "\n",
    "    if message is not None:\n",
    "        history_langchain_format.append(HumanMessage(content=message))\n",
    "        partial_message = \"\"\n",
    "        for response in llm.stream(history_langchain_format):\n",
    "            partial_message += response.content\n",
    "            yield partial_message\n",
    "\n",
    "\n",
    "demo_interface = gr.ChatInterface(\n",
    "    stream_response,\n",
    "    textbox=gr.Textbox(\n",
    "        placeholder=\"Send to the LLM...\",\n",
    "        container=False,\n",
    "        autoscroll=True,\n",
    "        scale=7\n",
    "    )\n",
    ")\n",
    "\n",
    "demo_interface.launch(share=True, debug=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1tq6h7yPmdX463xwA4hz31hen6Xh55MB-",
     "timestamp": 1733126702789
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
